{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wholeslidedata.annotation.wholeslideannotation import WholeSlideAnnotation\n",
    "from wholeslidedata.image.wholeslideimage import WholeSlideImage\n",
    "from wholeslidedata.annotation.types import PolygonAnnotation as Polygon\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "from py.helpers import get_outlines, get_area, get_patch, get_sub_areas, patch_empty, concat_one, BARRET_ROOT\n",
    "import os\n",
    "\n",
    "os.add_dll_directory(r'C:\\Program Files\\openslide-win64\\bin') # for openslide\n",
    "\n",
    "LANS_DIR = os.path.join(BARRET_ROOT, 'LANS_001-923')\n",
    "LANS_BIOP_ROOT = os.path.join(BARRET_ROOT, 'p53_biopsy-level_no-HE_Luuk', '_Luuk')\n",
    "LANS_BIOP_DIR = os.path.join(LANS_BIOP_ROOT, 'Slidescape_ASAP')\n",
    "BOLERO_DIR = os.path.join(BARRET_ROOT, 'BOLERO', 'P53 Bolero')\n",
    "PATHXL_DIR = os.path.join(LANS_BIOP_ROOT, 'PATHXL cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LANS_BIOP_DIR\n",
    "tiffs = [f for f in os.listdir(d) if f.endswith('.tiff')]\n",
    "xmls = [f for f in os.listdir(d) if f.endswith('.xml')]\n",
    "print(f'{d}: {len(tiffs)} tiffs, {len(xmls)} xmls')\n",
    "\n",
    "xml_names = [f.split('.')[0] for f in xmls]\n",
    "tiff_names = [f.split('.')[0] for f in tiffs]\n",
    "\n",
    "# Find the names that are in both lists\n",
    "both = set(xml_names).intersection(set(tiff_names))\n",
    "print(f'Both: {len(both)}')\n",
    "\n",
    "casenames = sorted(list(both))\n",
    "\n",
    "\n",
    "d = BOLERO_DIR\n",
    "tiffs = [f for f in os.listdir(d) if f.endswith('.tiff')]\n",
    "xmls = [f for f in os.listdir(d) if f.endswith('.xml')]\n",
    "print(f'{d}: {len(tiffs)} tiffs, {len(xmls)} xmls')\n",
    "\n",
    "xml_names = [f.split('.')[0] for f in xmls]\n",
    "tiff_names = [f.split('.')[0] for f in tiffs]\n",
    "\n",
    "# Find the names that are in both lists\n",
    "both = set(xml_names).intersection(set(tiff_names))\n",
    "print(f'Both: {len(both)}')\n",
    "\n",
    "casenames_BOLERO = sorted(list(both))\n",
    "\n",
    "\n",
    "d = PATHXL_DIR\n",
    "tiffs = [f for f in os.listdir(d) if f.endswith('.tiff')]\n",
    "xmls = [f for f in os.listdir(d) if f.endswith('.xml')]\n",
    "print(f'{d}: {len(tiffs)} tiffs, {len(xmls)} xmls')\n",
    "\n",
    "xml_names = [f.split('.')[0] for f in xmls]\n",
    "tiff_names = [f.split('.')[0] for f in tiffs]\n",
    "\n",
    "# Find the names that are in both lists\n",
    "both = set(xml_names).intersection(set(tiff_names))\n",
    "print(f'Both: {len(both)}')\n",
    "\n",
    "casenames_PATHXL = sorted(list(both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_annotated_cases(dir):\n",
    "    filepaths = {f.split('.')[0]:{\"wsi\":None, \"wsa\":None} for f in os.listdir(dir)}\n",
    "    for f in os.listdir(dir):\n",
    "        case = f.split('.')[0]\n",
    "\n",
    "        if \".tiff\" in f:\n",
    "            typ = \"wsi\"\n",
    "        elif f.endswith(\".xml\"):\n",
    "            typ = \"wsa\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        filepaths[case][typ] = os.path.join(dir, f)\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = get_all_annotated_cases(LANS_BIOP_DIR)\n",
    "cases_BOLERO = get_all_annotated_cases(BOLERO_DIR)\n",
    "cases_PATHXL = get_all_annotated_cases(PATHXL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sub_areas(wsi, sub_areas, area_labels=[], save_path=\"\", spacing=2.0, figsize_factor=2, show_emptiness=False):\n",
    "    nrows = len(sub_areas)\n",
    "    ncols = len(sub_areas[0])\n",
    "    fig, ax = plt.subplots(nrows,ncols, figsize=(ncols*figsize_factor,nrows*figsize_factor))\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            if ncols < 2:\n",
    "                index = i\n",
    "            else:\n",
    "                index = (i,j)\n",
    "\n",
    "            if j < len(sub_areas[i]):\n",
    "                sub_area = sub_areas[i][j]\n",
    "                sub_patch = wsi.get_patch(*sub_area, spacing)\n",
    "                \n",
    "                # color = \"red\" if sub_patch.mean() < 10 else \"black\"\n",
    "                # ax[i,j].text(105,128, f\"{sub_patch.std():.2f}\", c=color)\n",
    "                if show_emptiness:\n",
    "                    color = \"red\" if sub_patch.mean() > 223 else \"black\"\n",
    "                    ax[index].text(105,128, f\"{sub_patch.mean():.2f}\", c=color)\n",
    "                if len(area_labels) > 0:\n",
    "                    ax[index].set_title(area_labels[i*ncols+j])\n",
    "                ax[index].imshow(sub_patch)\n",
    "\n",
    "            ax[index].axis(\"off\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "# def save_all_sub_areas_plots(spacing, root=ROOT):\n",
    "#     for casename, case in tqdm(get_all_annotated_cases(root).items()):\n",
    "#         for coupe, paths in case.items():\n",
    "#             outlines = get_outlines(WholeSlideAnnotation(paths[\"wsa\"]))\n",
    "#             for biopsy_nr, outline in enumerate(outlines):\n",
    "#                 plot_sub_areas(\n",
    "#                     WholeSlideImage(paths[\"wsi\"]), \n",
    "#                     get_sub_areas(get_area(outline, spacing), spacing=spacing), \n",
    "#                     save_path=os.path.join(ROOT, \"visualisation\", f\"sub_areas_{casename}_{biopsy_nr}_{coupe}.png\"),\n",
    "#                     spacing=spacing)\n",
    "\n",
    "# save_all_sub_areas_plots(2, root=ROOT_ADJACENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 2.0\n",
    "\n",
    "d = LANS_BIOP_DIR\n",
    "casename = casenames[0]\n",
    "print(casename)\n",
    "casepaths = cases[casename]\n",
    "outlines = get_outlines(WholeSlideAnnotation(casepaths[\"wsa\"])) # biopsy outlines\n",
    "area = get_area(outlines[0], spacing) # biopsy area\n",
    "sub_areas = get_sub_areas(area) # sub areas (patches) of biopsy area\n",
    "wsi = WholeSlideImage(casepaths[\"wsi\"]) # whole slide image\n",
    "plot_sub_areas(wsi, sub_areas, show_emptiness=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of showing biopsies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 32.0\n",
    "\n",
    "d = LANS_BIOP_DIR\n",
    "casename = casenames[5]\n",
    "print(\"Dir: \", d)\n",
    "print(\"Case: \", casename)\n",
    "casepaths = cases[casename]\n",
    "wsa = WholeSlideAnnotation(casepaths[\"wsa\"])\n",
    "\n",
    "labels = [a.label.name for a in wsa.annotations] # biopsy labels\n",
    "\n",
    "outlines = get_outlines(wsa) # biopsy outlines\n",
    "areas = [get_area(outline, spacing) for outline in outlines] # biopsy areas\n",
    "# Append labels with (width, height) of biopsy\n",
    "labels = [f\"{label} ({int(area[2])}, {int(area[3])})\" for label, area in zip(labels, areas)]\n",
    "half_areas_len = int(np.ceil(len(areas)/2))\n",
    "areas = [[a for a in areas[:half_areas_len]], [a for a in areas[half_areas_len:]]]\n",
    "wsi = WholeSlideImage(casepaths[\"wsi\"]) # whole slide image\n",
    "plot_sub_areas(wsi, areas, figsize_factor=5, area_labels=labels, spacing=spacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather data on biopsies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacing = 0.5\n",
    "spacing  = 32.0\n",
    "\n",
    "if not CREATE_DATASET:\n",
    "    biopsies = {}\n",
    "\n",
    "    d = LANS_BIOP_DIR\n",
    "    print(\"Dir: \", d)\n",
    "    for casename in tqdm(casenames):    \n",
    "        casepaths = cases[casename]\n",
    "        wsa = WholeSlideAnnotation(casepaths[\"wsa\"])\n",
    "\n",
    "        labels = [a.label.name for a in wsa.annotations] # biopsy labels\n",
    "\n",
    "        outlines = get_outlines(wsa) # biopsy outlines\n",
    "\n",
    "        for b in range(len(labels)):\n",
    "            outline = outlines[b]\n",
    "            area = get_area(outline, spacing)\n",
    "\n",
    "            biopsies[f\"{casename}_b{b}\"] = {\n",
    "                \"dir\": d.split(\"\\\\\")[-1],\n",
    "                \"casename\": casename,\n",
    "                \"height\": int(area[2]),\n",
    "                \"width\": int(area[3]),\n",
    "                \"label\": labels[b],\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze biopsy distribution\n",
    "\n",
    "Largest biopsy: 21376 x 19328\twildtype\t413,155,328 pixels at 0.25 m/p\n",
    "\n",
    "5344 x 4832\twildtype\t25,822,208 at 1.0 m/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CREATE_DATASET:\n",
    "    df = pd.DataFrame.from_dict(biopsies, orient=\"index\")\n",
    "\n",
    "    # Filter out the \"exclude\" label biopsies, and the roi groups\n",
    "    df = df[df[\"label\"] != \"exclude\"]\n",
    "    df = df[df[\"label\"].apply(lambda x: \"roi\" not in x)]\n",
    "\n",
    "    df[\"pixels\"] = df[\"height\"] * df[\"width\"]\n",
    "\n",
    "    # Sort by pixels\n",
    "    df = df.sort_values(by=[\"pixels\"], ascending=False)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # Show barchart of label, with counts, using plt.bar\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(df[\"label\"].unique(), df[\"label\"].value_counts())\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Display counts on top of bars\n",
    "    for i, v in enumerate(df[\"label\"].value_counts()):\n",
    "        plt.text(i-0.2, v+1, str(v))\n",
    "    plt.show()\n",
    "\n",
    "    def get_first_letters(s):\n",
    "        \"\"\"Return every letter before the first non-alpha char. For example: 'RL1' -> 'RL'\"\"\"\n",
    "        for i, c in enumerate(s):\n",
    "            if not c.isalpha():\n",
    "                return s[:i]\n",
    "        return s\n",
    "\n",
    "    # Count how many RL or RBE numbers there are (in the casename), to make a barchart of that\n",
    "    df[\"RL VS RBE\"] = df[\"casename\"].apply(lambda x: get_first_letters(x))\n",
    "    # Group RBET and RBE together\n",
    "    df[\"RL VS RBE\"] = df[\"RL VS RBE\"].apply(lambda x: \"RBE\" if x == \"RBET\" else x)\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(df[\"RL VS RBE\"].unique(), df[\"RL VS RBE\"].value_counts())\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Display counts on top of bars\n",
    "    for i, v in enumerate(df[\"RL VS RBE\"].value_counts()):\n",
    "        plt.text(i-0.2, v+1, str(v))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Make a stacked barchart of RL VS RBE and label\n",
    "    df[\"count\"] = 1\n",
    "    df_stacked = df.groupby([\"RL VS RBE\", \"label\"]).sum()\n",
    "    df_stacked = df_stacked.reset_index()\n",
    "    df_stacked = df_stacked.pivot(index=\"RL VS RBE\", columns=\"label\", values=\"count\")\n",
    "    df_stacked = df_stacked.fillna(0)\n",
    "    df_stacked.plot.bar(stacked=True, figsize=(10,5))\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Display counts on top of each segment of the bars\n",
    "    for i, v in enumerate(df_stacked.values.flatten()):\n",
    "        ncols = len(df_stacked.columns)\n",
    "        bar_index = i // ncols\n",
    "        label_index = i % ncols\n",
    "        x = bar_index + label_index * 0.1\n",
    "        # Make sure the y value is based on the previous values in the stack of the same bar (so not the other bars)\n",
    "        y = sum(df_stacked.values.flatten()[bar_index*ncols:bar_index*ncols+i % len(df_stacked.columns)]) + v/2\n",
    "        plt.text(x-0.15, y, str(int(v)), ha=\"center\", va=\"center\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Make the same chart but now with percentages\n",
    "    df_stacked = df_stacked.div(df_stacked.sum(axis=1), axis=0)\n",
    "    df_stacked.plot.bar(stacked=True, figsize=(10,5))\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Display percentages on top of each segment of the bars\n",
    "    for i, v in enumerate(df_stacked.values.flatten()):\n",
    "        ncols = len(df_stacked.columns)\n",
    "        bar_index = i // ncols\n",
    "        label_index = i % ncols\n",
    "        x = bar_index\n",
    "        # Make sure the y value is based on the previous values in the stack of the same bar (so not the other bars)\n",
    "        y = sum(df_stacked.values.flatten()[bar_index*ncols:bar_index*ncols+i % len(df_stacked.columns)]) + v / 2\n",
    "        plt.text(x, y, f\"{v*100:.2f}%\", ha=\"center\", va=\"center\")\n",
    "    plt.show()\n",
    "\n",
    "    # Display rows with label none\n",
    "    print(\"Biopsies with label none: \")\n",
    "    none_labels = df[df[\"label\"] == \"none\"]\n",
    "    # Sort by casename\n",
    "    none_labels = none_labels.sort_values(by=[\"casename\"])\n",
    "    # Only unique casenames\n",
    "    none_labels = none_labels.drop_duplicates(subset=[\"casename\"])\n",
    "    display(none_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write txt file with casenames that have biopsies with label nullmutation or doubleclones\n",
    "if not CREATE_DATASET:\n",
    "    # Get all casenames with label nullmutation or doubleclones\n",
    "    casenames_nullmutation_doubleclones = df[(df[\"label\"] == \"nullmutation\") | (df[\"label\"] == \"doubleclones\")][\"casename\"].unique()\n",
    "    \n",
    "    # Sort\n",
    "    casenames_nullmutation_doubleclones = sorted(casenames_nullmutation_doubleclones)\n",
    "\n",
    "    with open(os.path.join(LANS_BIOP_ROOT, \"casenames_nullmutation_doubleclones.txt\"), \"w\") as f:\n",
    "        for casename in casenames_nullmutation_doubleclones:\n",
    "            f.write(f\"{casename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CREATE_DATASET:\n",
    "    max_size = 9000\n",
    "    max_size = 128\n",
    "\n",
    "    # Select cases with height and width < 9000\n",
    "    small_biopsies = df[(df[\"height\"] < max_size) & (df[\"width\"] < max_size)]\n",
    "\n",
    "    display(small_biopsies)\n",
    "\n",
    "    # Same plot as before, but now with small biopsies\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(small_biopsies[\"label\"].value_counts().keys(), small_biopsies[\"label\"].value_counts())\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    for i, v in enumerate(small_biopsies[\"label\"].value_counts()):\n",
    "        plt.text(i-0.2, v+1, str(v))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 1.0\n",
    "\n",
    "CREATE_DATASET = True\n",
    "if CREATE_DATASET:\n",
    "    # Iterate through all cases and save the biopsies as pngs\n",
    "    # Also save a csv with the labels of the biopsies\n",
    "    destination = os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\") # Dir for results\n",
    "    dest_biop = os.path.join(destination, f\"biopsies_s{spacing}\") # Dir for biopsy images\n",
    "    os.makedirs(dest_biop, exist_ok=True)\n",
    "\n",
    "    biopsy_df = {}\n",
    "    errors = []\n",
    "\n",
    "    d = LANS_BIOP_DIR # Dir with biopsies\n",
    "    print(\"Dir: \", d)\n",
    "    for casename in tqdm(casenames):\n",
    "        casepaths = cases[casename]\n",
    "        wsa = WholeSlideAnnotation(casepaths[\"wsa\"])\n",
    "\n",
    "        labels = [a.label.name for a in wsa.annotations] # biopsy labels\n",
    "\n",
    "        outlines = get_outlines(wsa) # biopsy outlines\n",
    "        try:\n",
    "            wsi = WholeSlideImage(casepaths[\"wsi\"]) # whole slide image\n",
    "        except Exception as e:\n",
    "            errors.append((casepaths[\"wsi\"], e))\n",
    "            continue\n",
    "\n",
    "        for b in range(len(labels)):\n",
    "            outline = outlines[b]\n",
    "            area = get_area(outline, spacing)\n",
    "\n",
    "            biopsy_name = f\"{casename}_b{b}\"\n",
    "            if labels[b] == \"none\":\n",
    "                continue\n",
    "            biopsy_df[biopsy_name] = {\n",
    "                \"dir\": d,\n",
    "                \"casename\": casename,\n",
    "                \"height\": int(area[2]),\n",
    "                \"width\": int(area[3]),\n",
    "                \"label\": labels[b],\n",
    "            }\n",
    "\n",
    "            if os.path.exists(os.path.join(dest_biop, f\"{biopsy_name}.png\")):\n",
    "                continue\n",
    "\n",
    "            # Save biopsy with cv2\n",
    "            biopsy = wsi.get_patch(*area, spacing)\n",
    "\n",
    "            # Convert BGR to RGB\n",
    "            biopsy = cv2.cvtColor(biopsy, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(os.path.join(dest_biop, f\"{biopsy_name}.png\"), biopsy)\n",
    "\n",
    "    print(\"Errors: \", len(errors))\n",
    "    display(errors)\n",
    "\n",
    "    biopsy_df = pd.DataFrame.from_dict(biopsy_df, orient=\"index\")\n",
    "    biopsy_df[\"pixels\"] = biopsy_df[\"height\"] * biopsy_df[\"width\"]\n",
    "\n",
    "    # Save to csv\n",
    "    biopsy_df.to_csv(os.path.join(destination, f\"biopsy_labels_s{spacing}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of casenames with BIG in it\n",
    "print(\"BIG: \", sum([1 for c in casenames if \"BIG\" in c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "errors: 19, each and every BIG tiff\n",
    "\n",
    " clearly something about the BIG tiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOLERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 1.0\n",
    "\n",
    "CREATE_DATASET = True\n",
    "if CREATE_DATASET:\n",
    "    # Iterate through all cases and save the biopsies as pngs\n",
    "    # Also save a csv with the labels of the biopsies\n",
    "    results_dirname = \"dataset_fullsize_BOLERO\"\n",
    "    destination = os.path.join(LANS_BIOP_ROOT, results_dirname) # Dir for results\n",
    "    dest_biop = os.path.join(destination, f\"biopsies_s{spacing}\") # Dir for biopsy images\n",
    "    os.makedirs(dest_biop, exist_ok=True)\n",
    "\n",
    "    biopsy_df = {}\n",
    "    errors = []\n",
    "\n",
    "    d = BOLERO_DIR # Dir with biopsies\n",
    "    print(\"Dir: \", d)\n",
    "    for casename in tqdm(casenames_BOLERO):\n",
    "        casepaths = cases_BOLERO[casename]\n",
    "        wsa = WholeSlideAnnotation(casepaths[\"wsa\"])\n",
    "\n",
    "        outlines = get_outlines(wsa) # biopsy outlines\n",
    "        try:\n",
    "            wsi = WholeSlideImage(casepaths[\"wsi\"]) # whole slide image\n",
    "        except Exception as e:\n",
    "            errors.append((casepaths[\"wsi\"], e))\n",
    "            continue\n",
    "\n",
    "        for b in range(len(outlines)):\n",
    "            outline = outlines[b]\n",
    "            area = get_area(outline, spacing)\n",
    "\n",
    "            biopsy_name = f\"{casename}_b{b}\"\n",
    "            biopsy_df[biopsy_name] = {\n",
    "                \"dir\": d,\n",
    "                \"casename\": casename,\n",
    "                \"height\": int(area[2]),\n",
    "                \"width\": int(area[3]),\n",
    "            }\n",
    "\n",
    "            if os.path.exists(os.path.join(dest_biop, f\"{biopsy_name}.png\")):\n",
    "                continue\n",
    "\n",
    "            # Save biopsy with cv2\n",
    "            biopsy = wsi.get_patch(*area, spacing)\n",
    "\n",
    "            # Convert BGR to RGB\n",
    "            biopsy = cv2.cvtColor(biopsy, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(os.path.join(dest_biop, f\"{biopsy_name}.png\"), biopsy)\n",
    "\n",
    "    print(\"Errors: \", len(errors))\n",
    "    display(errors)\n",
    "\n",
    "    biopsy_df = pd.DataFrame.from_dict(biopsy_df, orient=\"index\")\n",
    "    biopsy_df[\"pixels\"] = biopsy_df[\"height\"] * biopsy_df[\"width\"]\n",
    "\n",
    "    # Save to csv\n",
    "    biopsy_df.to_csv(os.path.join(destination, f\"biopsy_labels_s{spacing}.csv\"))\n",
    "\n",
    "\n",
    "    # Also copy the biopsies to a new folder, and rename them to the number of the row in the csv (so 0.png, 1.png etc.)\n",
    "    print(\"spacing: \", spacing)\n",
    "\n",
    "    # Use the saved csv to make a prepared dataset, with one folder containing all biopsies with names like 0_0.png 1_2.png etc., and a csv with the labels\n",
    "    # Map each casename to a number\n",
    "    mapping = {casename: i for i, casename in enumerate(biopsy_df[\"casename\"].unique())}\n",
    "\n",
    "    # For every file in biopsies_s{spacing}, copy it to biopsies_s{spacing}_anon\n",
    "    source = os.path.join(LANS_BIOP_ROOT, results_dirname, f\"biopsies_s{spacing}\")\n",
    "    destination = os.path.join(LANS_BIOP_ROOT, results_dirname, f\"biopsies_s{spacing}_anon\")\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "    # Enumerate over the rows in the csv, NOT the index\n",
    "    df_clean = biopsy_df.copy().reset_index()\n",
    "    for i, row in tqdm(df_clean.iterrows(), total=len(biopsy_df)):\n",
    "        casename = row[\"casename\"]\n",
    "        # Get the biopsy name\n",
    "        biopsy_name = row[\"index\"]\n",
    "        biopsy_nr = int(biopsy_name.split(\"_b\")[-1])\n",
    "        # Get the source path\n",
    "        source_path = os.path.join(source, f\"{biopsy_name}.png\")\n",
    "        # Get the destination path\n",
    "        destination_path = os.path.join(destination, f\"{mapping[casename]}_{biopsy_nr}.png\")\n",
    "        # Copy the file\n",
    "        os.system(f'copy \"{source_path}\" \"{destination_path}\"')\n",
    "\n",
    "    # Drop all unnecessary columns, only keeping the index and the label\n",
    "    df_clean = df_clean.drop(columns=[\"dir\", \"casename\", \"height\", \"width\", \"pixels\", \"index\"])\n",
    "\n",
    "    # Name the index column \"id\" and then don't save the index\n",
    "    df_clean.index.name = \"id\"\n",
    "    df_clean.to_csv(os.path.join(LANS_BIOP_ROOT, results_dirname, f\"biopsy_labels_anon_s{spacing}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels for pathxl dataset\n",
    "labels = pd.read_csv(os.path.join(LANS_BIOP_ROOT, 'pathxl_study_slide_labels.csv'))\n",
    "display(labels.head())\n",
    "pathxl_case_nrs = labels['case_nr'].to_list()\n",
    "print(pathxl_case_nrs[:5])\n",
    "print(len(pathxl_case_nrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# First we need to go through all XMLs that contain the annotations\n",
    "for casename in tqdm(casenames_PATHXL):\n",
    "    casepaths = cases_PATHXL[casename]\n",
    "\n",
    "    # Open this XML file as text\n",
    "    with open(casepaths['wsa'], 'r') as f:\n",
    "        xml = f.read()\n",
    "\n",
    "    # Look for all name=\"annotation {number}\" in the XML, because we need to check if they're in order\n",
    "    # Not cap sensitive\n",
    "    annotations = re.findall(r'name=\"annotation (\\d+)\"', xml, re.IGNORECASE) # This will return a list of strings\n",
    "    annotations = [int(a) for a in annotations] # Convert the strings to integers\n",
    "\n",
    "    # Check if the annotations are in order\n",
    "    sorted_annotations = sorted(annotations)\n",
    "    if annotations != sorted_annotations:\n",
    "        print(f\"Annotations are not in order for {casename}: {annotations}\")\n",
    "    if len(annotations) == 0:\n",
    "        print(f\"No annotations found for {casename}\")\n",
    "        continue\n",
    "    if annotations[0] != 1:\n",
    "        print(f\"First annotation is not 1 for {casename}: {annotations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'name=\"annotation 5\"  name=\"annotation 2\"   name=\"annotation 3\"  name=\"annotation 1\"'\n",
    "\n",
    "annotations = re.findall(r'name=\"annotation (\\d+)\"', string, re.IGNORECASE) # This will return a list of strings\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 1.0\n",
    "\n",
    "CREATE_DATASET = True\n",
    "if CREATE_DATASET:\n",
    "    # Iterate through all cases and save the biopsies as pngs\n",
    "    # Also save a csv with the labels of the biopsies\n",
    "    results_dirname = \"dataset_fullsize_PATHXL\"\n",
    "    destination = os.path.join(LANS_BIOP_ROOT, results_dirname) # Dir for results\n",
    "    dest_biop = os.path.join(destination, f\"biopsies_s{spacing}\") # Dir for biopsy images\n",
    "    os.makedirs(dest_biop, exist_ok=True)\n",
    "\n",
    "    biopsy_df = {}\n",
    "    errors = []\n",
    "\n",
    "    d = PATHXL_DIR # Dir with biopsies\n",
    "    print(\"Dir: \", d)\n",
    "    for casename in tqdm(casenames_PATHXL):\n",
    "        casepaths = cases_PATHXL[casename]\n",
    "        wsa = WholeSlideAnnotation(casepaths[\"wsa\"])\n",
    "\n",
    "        labels = [a.label.name for a in wsa.annotations] # biopsy labels\n",
    "        if len(labels) == 0:\n",
    "            continue\n",
    "\n",
    "        outlines = get_outlines(wsa) # biopsy outlines\n",
    "        try:\n",
    "            wsi = WholeSlideImage(casepaths[\"wsi\"]) # whole slide image\n",
    "        except Exception as e:\n",
    "            errors.append((casepaths[\"wsi\"], e))\n",
    "            continue\n",
    "\n",
    "        for b in range(len(outlines)):\n",
    "            outline = outlines[b]\n",
    "            area = get_area(outline, spacing)\n",
    "\n",
    "            biopsy_name = f\"{casename}_b{b+1}\"\n",
    "            biopsy_df[biopsy_name] = {\n",
    "                \"dir\": d,\n",
    "                \"casename\": casename,\n",
    "                \"height\": int(area[2]),\n",
    "                \"width\": int(area[3]),\n",
    "            }\n",
    "\n",
    "            if os.path.exists(os.path.join(dest_biop, f\"{biopsy_name}.png\")):\n",
    "                continue\n",
    "\n",
    "            # Save biopsy with cv2\n",
    "            biopsy = wsi.get_patch(*area, spacing)\n",
    "\n",
    "            # Convert BGR to RGB\n",
    "            biopsy = cv2.cvtColor(biopsy, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(os.path.join(dest_biop, f\"{biopsy_name}.png\"), biopsy)\n",
    "\n",
    "    print(\"Errors: \", len(errors))\n",
    "    display(errors)\n",
    "\n",
    "    biopsy_df = pd.DataFrame.from_dict(biopsy_df, orient=\"index\")\n",
    "    biopsy_df[\"pixels\"] = biopsy_df[\"height\"] * biopsy_df[\"width\"]\n",
    "\n",
    "    # Save to csv\n",
    "    biopsy_df.to_csv(os.path.join(destination, f\"biopsy_labels_s{spacing}.csv\"))\n",
    "\n",
    "\n",
    "    # Also copy the biopsies to a new folder, and rename them to the number of the row in the csv (so 0.png, 1.png etc.)\n",
    "    print(\"spacing: \", spacing)\n",
    "\n",
    "    # Use the saved csv to make a prepared dataset, with one folder containing all biopsies with names like 0_0.png 1_2.png etc., and a csv with the labels\n",
    "    # Map each casename to a number\n",
    "    mapping = {casename: i for i, casename in enumerate(biopsy_df[\"casename\"].unique())}\n",
    "\n",
    "    # For every file in biopsies_s{spacing}, copy it to biopsies_s{spacing}_anon\n",
    "    source = os.path.join(LANS_BIOP_ROOT, results_dirname, f\"biopsies_s{spacing}\")\n",
    "    destination = os.path.join(LANS_BIOP_ROOT, results_dirname, f\"biopsies_s{spacing}_anon\")\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "    # Enumerate over the rows in the csv, NOT the index\n",
    "    df_clean = biopsy_df.copy().reset_index()\n",
    "    for i, row in tqdm(df_clean.iterrows(), total=len(biopsy_df)):\n",
    "        casename = row[\"casename\"]\n",
    "        # Get the biopsy name\n",
    "        biopsy_name = row[\"index\"]\n",
    "        biopsy_nr = int(biopsy_name.split(\"_b\")[-1])\n",
    "        # Get the source path\n",
    "        source_path = os.path.join(source, f\"{biopsy_name}.png\")\n",
    "        # Get the destination path\n",
    "        destination_path = os.path.join(destination, f\"{mapping[casename]}_{biopsy_nr}.png\")\n",
    "        # Copy the file\n",
    "        os.system(f'copy \"{source_path}\" \"{destination_path}\"')\n",
    "\n",
    "    # Drop all unnecessary columns, only keeping the index and the label\n",
    "    df_clean = df_clean.drop(columns=[\"dir\", \"casename\", \"height\", \"width\", \"pixels\", \"index\"])\n",
    "\n",
    "    # Name the index column \"id\" and then don't save the index\n",
    "    df_clean.index.name = \"id\"\n",
    "    df_clean.to_csv(os.path.join(LANS_BIOP_ROOT, results_dirname, f\"biopsy_labels_anon_s{spacing}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"wildtype\": 0,\n",
    "    \"overexpression\": 1,\n",
    "    \"nullmutation\": 2,\n",
    "    \"doubleclones\": 3,\n",
    "    \"none\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_DATASET:\n",
    "    # Iterate through csv that was just created, and map the labels to numbers\n",
    "    # Also copy the biopsies to a new folder, and rename them to the number of the row in the csv (so 0.png, 1.png etc.)\n",
    "    print(\"spacing: \", spacing)\n",
    "\n",
    "    # Use the saved csv to make a prepared dataset, with one folder containing all biopsies with names like 0.png 1.png etc., and a csv with the labels\n",
    "    # Open the csv\n",
    "    df = pd.read_csv(os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", f\"biopsy_labels_s{spacing}.csv\"), index_col=0)\n",
    "\n",
    "    # For every file in biopsies_s4.0, copy it to biopsies_s4.0_anon, and rename it to the number of the row in the csv (so 0.png, 1.png etc.) (NOT the index)\n",
    "    source = os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", f\"biopsies_s{spacing}\")\n",
    "    destination = os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", f\"biopsies_s{spacing}_anon\")\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "    # Enumerate over the rows in the csv, NOT the index\n",
    "    df_clean = df.copy().reset_index()\n",
    "    for i, row in tqdm(df_clean.iterrows(), total=len(df)):\n",
    "        # Get the biopsy name\n",
    "        biopsy_name = row[\"index\"]\n",
    "        # Get the label\n",
    "        label = row[\"label\"]\n",
    "        # Get the source path\n",
    "        source_path = os.path.join(source, f\"{biopsy_name}.png\")\n",
    "        # Get the destination path\n",
    "        destination_path = os.path.join(destination, f\"{i}.png\")\n",
    "        # Copy the file\n",
    "        os.system(f'copy \"{source_path}\" \"{destination_path}\"')\n",
    "\n",
    "    # Drop all unnecessary columns, only keeping the index and the label\n",
    "    df_clean = df_clean.drop(columns=[\"dir\", \"casename\", \"height\", \"width\", \"pixels\", \"index\"])\n",
    "\n",
    "    # Map labels to numbers according to the mapping:\n",
    "    df_clean[\"label\"] = df_clean[\"label\"].map(mapping)\n",
    "    # It should be an integer\n",
    "    df_clean[\"label\"] = df_clean[\"label\"].astype(int)\n",
    "\n",
    "    # Name the index column \"id\" and then don't save the index\n",
    "    df_clean.index.name = \"id\"\n",
    "    df_clean.to_csv(os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", f\"biopsy_labels_anon_s{spacing}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", f\"biopsies_s{spacing}_anon\")\n",
    "saved = set(sorted([int(f.split('.')[0]) for f in os.listdir(destination)]))\n",
    "\n",
    "# Check for missing files\n",
    "required = set(range(len(df)))\n",
    "missing = list(required.difference(saved))\n",
    "# Give each row of df a number as index, because right now it's the biopsy name\n",
    "df_numeric_index = df.reset_index()\n",
    "print(\"Missing: \", df_numeric_index.loc[missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch data\n",
    "\n",
    "not using this; do this during or just before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of patching\n",
    "spacing = 2.0\n",
    "\n",
    "d = LANS_BIOP_DIR\n",
    "casename = casenames[0]\n",
    "print(casename)\n",
    "casepaths = cases[casename]\n",
    "outlines = get_outlines(WholeSlideAnnotation(casepaths[\"wsa\"])) # biopsy outlines\n",
    "area = get_area(outlines[0], spacing) # biopsy area\n",
    "sub_areas = get_sub_areas(area) # sub areas (patches) of biopsy area\n",
    "wsi = WholeSlideImage(casepaths[\"wsi\"]) # whole slide image\n",
    "plot_sub_areas(wsi, sub_areas, show_emptiness=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to split each biopsy into patches of 256x256, and give them a label according to the biopsy label\n",
    "# The patches should be saved in a folder called patches_s{spacing}_anon, with names like 0.png, 1.png etc.\n",
    "# The labels should be saved in a csv called patches_s{spacing}_anon.csv\n",
    "# The csv should have the following columns: id, label\n",
    "# The id should be the number of the patch (so 0, 1 etc.)\n",
    "# The label should be a number according to the mapping above (so 0, 1, 2, 3 for wildtype, overexpression, nullmutation, doubleclones respectively)\n",
    "spacing = 2.0\n",
    "\n",
    "# CREATE_DATASET = True\n",
    "if CREATE_DATASET:\n",
    "    # Iterate through all cases and save patches of the biopsies as pngs\n",
    "    # Also save a csv with the labels of the patches\n",
    "    errors = []\n",
    "\n",
    "    destination_dir = os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", f\"patches_s{spacing}_anon\")\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "    patch_labels = {}\n",
    "\n",
    "    ctr = 0\n",
    "    # Iterate through every case\n",
    "    for casename in tqdm(casenames):\n",
    "        # Get the casepaths\n",
    "        casepaths = cases[casename]\n",
    "        # Get the wsa\n",
    "        wsa = WholeSlideAnnotation(casepaths[\"wsa\"])\n",
    "        # Get the labels\n",
    "        labels = [a.label.name for a in wsa.annotations]\n",
    "        # Get the outlines\n",
    "        outlines = get_outlines(wsa)\n",
    "        # Get the wsi\n",
    "        try:\n",
    "            wsi = WholeSlideImage(casepaths[\"wsi\"])\n",
    "        except Exception as e:\n",
    "            errors.append((casepaths[\"wsi\"], e))\n",
    "            continue\n",
    "\n",
    "        # Iterate through every biopsy\n",
    "        for b in range(len(labels)):\n",
    "            # Get the outline\n",
    "            outline = outlines[b]\n",
    "            # Get the area\n",
    "            area = get_area(outline, spacing)\n",
    "            # Get the sub areas\n",
    "            sub_areas = get_sub_areas(area)\n",
    "            # Flatten the sub areas\n",
    "            sub_areas = [a for sub_area in sub_areas for a in sub_area]\n",
    "            # Get the label\n",
    "            label = labels[b]\n",
    "            if label == \"none\":\n",
    "                continue\n",
    "\n",
    "            # Iterate through every sub area\n",
    "            for i, sub_area in enumerate(sub_areas):\n",
    "                # Get the patch\n",
    "                patch = wsi.get_patch(*sub_area, spacing)\n",
    "                # Get the destination path\n",
    "                destination_path = os.path.join(destination_dir, f\"{ctr}.png\")\n",
    "                # Save the patch in RGB format\n",
    "                cv2.imwrite(destination_path, patch[...,::-1])\n",
    "                # Add the label to the patch_labels dict\n",
    "                patch_labels[ctr] = mapping[label]\n",
    "                # Increment the counter\n",
    "                ctr += 1\n",
    "\n",
    "    patch_labels = pd.DataFrame.from_dict(patch_labels, orient=\"index\")\n",
    "    patch_labels.index.name = \"id\"\n",
    "    patch_labels.columns = [\"label\"]\n",
    "    patch_labels.to_csv(os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", f\"patches_s{spacing}_anon.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
