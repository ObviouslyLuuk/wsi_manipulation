{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wholeslidedata.annotation.wholeslideannotation import WholeSlideAnnotation\n",
    "from wholeslidedata.image.wholeslideimage import WholeSlideImage\n",
    "from wholeslidedata.annotation.types import PolygonAnnotation as Polygon\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "from py.helpers import get_outlines, get_area, get_patch, get_sub_areas, patch_empty, concat_one, BARRET_ROOT\n",
    "import os\n",
    "\n",
    "os.add_dll_directory(r'C:\\Program Files\\openslide-win64\\bin') # for openslide\n",
    "\n",
    "LANS_DIR = os.path.join(BARRET_ROOT, 'LANS_001-923')\n",
    "LANS_BIOP_ROOT = os.path.join(BARRET_ROOT, 'p53_experiment_luuk_biopsy level_no HE')\n",
    "LANS_BIOP_DIR = os.path.join(LANS_BIOP_ROOT, 'P53_score_high_consensus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LANS_BIOP_DIR\n",
    "tiffs = [f for f in os.listdir(d) if f.endswith('.tiff')]\n",
    "xmls = [f for f in os.listdir(d) if f.endswith('.xml')]\n",
    "print(f'{d}: {len(tiffs)} tiffs, {len(xmls)} xmls')\n",
    "\n",
    "xml_names = [f.split('.')[0] for f in xmls]\n",
    "tiff_names = [f.split('.')[0] for f in tiffs]\n",
    "\n",
    "# Find the names that are in both lists\n",
    "both = set(xml_names).intersection(set(tiff_names))\n",
    "print(f'Both: {len(both)}')\n",
    "\n",
    "casenames = sorted(list(both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_annotated_cases(dir):\n",
    "    filepaths = {f.split('.')[0]:{\"wsi\":None, \"wsa\":None} for f in os.listdir(dir)}\n",
    "    for f in os.listdir(dir):\n",
    "        case = f.split('.')[0]\n",
    "\n",
    "        if \".tiff\" in f:\n",
    "            typ = \"wsi\"\n",
    "        elif f.endswith(\".xml\"):\n",
    "            typ = \"wsa\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        filepaths[case][typ] = os.path.join(dir, f)\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = get_all_annotated_cases(LANS_BIOP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sub_areas(wsi, sub_areas, area_labels=[], save_path=\"\", spacing=2.0, figsize_factor=2, show_emptiness=False):\n",
    "    nrows = len(sub_areas)\n",
    "    ncols = len(sub_areas[0])\n",
    "    fig, ax = plt.subplots(nrows,ncols, figsize=(ncols*figsize_factor,nrows*figsize_factor))\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            if ncols < 2:\n",
    "                index = i\n",
    "            else:\n",
    "                index = (i,j)\n",
    "\n",
    "            if j < len(sub_areas[i]):\n",
    "                sub_area = sub_areas[i][j]\n",
    "                sub_patch = wsi.get_patch(*sub_area, spacing)\n",
    "                \n",
    "                # color = \"red\" if sub_patch.mean() < 10 else \"black\"\n",
    "                # ax[i,j].text(105,128, f\"{sub_patch.std():.2f}\", c=color)\n",
    "                if show_emptiness:\n",
    "                    color = \"red\" if sub_patch.mean() > 223 else \"black\"\n",
    "                    ax[index].text(105,128, f\"{sub_patch.mean():.2f}\", c=color)\n",
    "                if len(area_labels) > 0:\n",
    "                    ax[index].set_title(area_labels[i*ncols+j])\n",
    "                ax[index].imshow(sub_patch)\n",
    "\n",
    "            ax[index].axis(\"off\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "# def save_all_sub_areas_plots(spacing, root=ROOT):\n",
    "#     for casename, case in tqdm(get_all_annotated_cases(root).items()):\n",
    "#         for coupe, paths in case.items():\n",
    "#             outlines = get_outlines(WholeSlideAnnotation(paths[\"wsa\"]))\n",
    "#             for biopsy_nr, outline in enumerate(outlines):\n",
    "#                 plot_sub_areas(\n",
    "#                     WholeSlideImage(paths[\"wsi\"]), \n",
    "#                     get_sub_areas(get_area(outline, spacing), spacing=spacing), \n",
    "#                     save_path=os.path.join(ROOT, \"visualisation\", f\"sub_areas_{casename}_{biopsy_nr}_{coupe}.png\"),\n",
    "#                     spacing=spacing)\n",
    "\n",
    "# save_all_sub_areas_plots(2, root=ROOT_ADJACENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 2.0\n",
    "\n",
    "d = LANS_BIOP_DIR\n",
    "casename = casenames[0]\n",
    "print(casename)\n",
    "casepaths = cases[casename]\n",
    "outlines = get_outlines(WholeSlideAnnotation(casepaths[\"wsa\"])) # biopsy outlines\n",
    "area = get_area(outlines[0], spacing) # biopsy area\n",
    "sub_areas = get_sub_areas(area) # sub areas (patches) of biopsy area\n",
    "wsi = WholeSlideImage(casepaths[\"wsi\"]) # whole slide image\n",
    "plot_sub_areas(wsi, sub_areas, show_emptiness=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of showing biopsies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 32.0\n",
    "\n",
    "d = LANS_BIOP_DIR\n",
    "casename = casenames[5]\n",
    "print(\"Dir: \", d)\n",
    "print(\"Case: \", casename)\n",
    "casepaths = cases[casename]\n",
    "wsa = WholeSlideAnnotation(casepaths[\"wsa\"])\n",
    "\n",
    "labels = [a.label.name for a in wsa.annotations] # biopsy labels\n",
    "\n",
    "outlines = get_outlines(wsa) # biopsy outlines\n",
    "areas = [get_area(outline, spacing) for outline in outlines] # biopsy areas\n",
    "# Append labels with (width, height) of biopsy\n",
    "labels = [f\"{label} ({int(area[2])}, {int(area[3])})\" for label, area in zip(labels, areas)]\n",
    "half_areas_len = int(np.ceil(len(areas)/2))\n",
    "areas = [[a for a in areas[:half_areas_len]], [a for a in areas[half_areas_len:]]]\n",
    "wsi = WholeSlideImage(casepaths[\"wsi\"]) # whole slide image\n",
    "plot_sub_areas(wsi, areas, figsize_factor=5, area_labels=labels, spacing=spacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather data on biopsies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacing = 0.5\n",
    "spacing  = 32.0\n",
    "\n",
    "if not CREATE_DATASET:\n",
    "    biopsies = {}\n",
    "\n",
    "    d = LANS_BIOP_DIR\n",
    "    print(\"Dir: \", d)\n",
    "    for casename in tqdm(casenames):    \n",
    "        casepaths = cases[casename]\n",
    "        wsa = WholeSlideAnnotation(casepaths[\"wsa\"])\n",
    "\n",
    "        labels = [a.label.name for a in wsa.annotations] # biopsy labels\n",
    "\n",
    "        outlines = get_outlines(wsa) # biopsy outlines\n",
    "\n",
    "        for b in range(len(labels)):\n",
    "            outline = outlines[b]\n",
    "            area = get_area(outline, spacing)\n",
    "\n",
    "            biopsies[f\"{casename}_b{b}\"] = {\n",
    "                \"dir\": d.split(\"\\\\\")[-1],\n",
    "                \"casename\": casename,\n",
    "                \"height\": int(area[2]),\n",
    "                \"width\": int(area[3]),\n",
    "                \"label\": labels[b],\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze biopsy distribution\n",
    "\n",
    "Largest biopsy: 21376 x 19328\twildtype\t413,155,328 pixels at 0.25 m/p\n",
    "\n",
    "5344 x 4832\twildtype\t25,822,208 at 1.0 m/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CREATE_DATASET:\n",
    "    df = pd.DataFrame.from_dict(biopsies, orient=\"index\")\n",
    "    df[\"pixels\"] = df[\"height\"] * df[\"width\"]\n",
    "\n",
    "    # Sort by pixels\n",
    "    df = df.sort_values(by=[\"pixels\"], ascending=False)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # Show barchart of label, with counts, using plt.bar\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(df[\"label\"].unique(), df[\"label\"].value_counts())\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Display counts on top of bars\n",
    "    for i, v in enumerate(df[\"label\"].value_counts()):\n",
    "        plt.text(i-0.2, v+1, str(v))\n",
    "    plt.show()\n",
    "\n",
    "    def get_first_letters(s):\n",
    "        \"\"\"Return every letter before the first non-alpha char. For example: 'RL1' -> 'RL'\"\"\"\n",
    "        for i, c in enumerate(s):\n",
    "            if not c.isalpha():\n",
    "                return s[:i]\n",
    "        return s\n",
    "\n",
    "    # Count how many RL or RBE numbers there are (in the casename), to make a barchart of that\n",
    "    df[\"RL VS RBE\"] = df[\"casename\"].apply(lambda x: get_first_letters(x))\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(df[\"RL VS RBE\"].unique(), df[\"RL VS RBE\"].value_counts())\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Display counts on top of bars\n",
    "    for i, v in enumerate(df[\"RL VS RBE\"].value_counts()):\n",
    "        plt.text(i-0.2, v+1, str(v))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Make a stacked barchart of RL VS RBE and label\n",
    "    df[\"count\"] = 1\n",
    "    df_stacked = df.groupby([\"RL VS RBE\", \"label\"]).sum()\n",
    "    df_stacked = df_stacked.reset_index()\n",
    "    df_stacked = df_stacked.pivot(index=\"RL VS RBE\", columns=\"label\", values=\"count\")\n",
    "    df_stacked = df_stacked.fillna(0)\n",
    "    df_stacked.plot.bar(stacked=True, figsize=(10,5))\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Display counts on top of each segment of the bars\n",
    "    for i, v in enumerate(df_stacked.values.flatten()):\n",
    "        ncols = len(df_stacked.columns)\n",
    "        bar_index = i // ncols\n",
    "        label_index = i % ncols\n",
    "        x = bar_index + label_index * 0.1\n",
    "        # Make sure the y value is based on the previous values in the stack of the same bar (so not the other bars)\n",
    "        y = sum(df_stacked.values.flatten()[bar_index*ncols:bar_index*ncols+i % len(df_stacked.columns)]) + v/2\n",
    "        plt.text(x-0.15, y, str(int(v)), ha=\"center\", va=\"center\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Make the same chart but now with percentages\n",
    "    df_stacked = df_stacked.div(df_stacked.sum(axis=1), axis=0)\n",
    "    df_stacked.plot.bar(stacked=True, figsize=(10,5))\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Display percentages on top of each segment of the bars\n",
    "    for i, v in enumerate(df_stacked.values.flatten()):\n",
    "        ncols = len(df_stacked.columns)\n",
    "        bar_index = i // ncols\n",
    "        label_index = i % ncols\n",
    "        x = bar_index\n",
    "        # Make sure the y value is based on the previous values in the stack of the same bar (so not the other bars)\n",
    "        y = sum(df_stacked.values.flatten()[bar_index*ncols:bar_index*ncols+i % len(df_stacked.columns)]) + v / 2\n",
    "        plt.text(x, y, f\"{v*100:.2f}%\", ha=\"center\", va=\"center\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # # Display rows with label none (They're all solved now :) )\n",
    "    # none_labels = df[df[\"label\"] == \"none\"]\n",
    "    # # Sort by casename\n",
    "    # none_labels = none_labels.sort_values(by=[\"casename\"])\n",
    "    # # Only unique casenames\n",
    "    # none_labels = none_labels.drop_duplicates(subset=[\"casename\"])\n",
    "    # display(none_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CREATE_DATASET:\n",
    "    max_size = 9000\n",
    "    max_size = 128\n",
    "\n",
    "    # Select cases with height and width < 9000\n",
    "    small_biopsies = df[(df[\"height\"] < max_size) & (df[\"width\"] < max_size)]\n",
    "\n",
    "    display(small_biopsies)\n",
    "\n",
    "    # Same plot as before, but now with small biopsies\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(small_biopsies[\"label\"].value_counts().keys(), small_biopsies[\"label\"].value_counts())\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    for i, v in enumerate(small_biopsies[\"label\"].value_counts()):\n",
    "        plt.text(i-0.2, v+1, str(v))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 32.0\n",
    "\n",
    "CREATE_DATASET = True\n",
    "if CREATE_DATASET:\n",
    "    destination = os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\")\n",
    "    dest_biop = os.path.join(destination, f\"biopsies_s{spacing}\")\n",
    "    os.makedirs(dest_biop, exist_ok=True)\n",
    "\n",
    "    biopsy_df = {}\n",
    "    errors = []\n",
    "\n",
    "    d = LANS_BIOP_DIR\n",
    "    print(\"Dir: \", d)\n",
    "    for casename in tqdm(casenames):\n",
    "        casepaths = cases[casename]\n",
    "        wsa = WholeSlideAnnotation(casepaths[\"wsa\"])\n",
    "\n",
    "        labels = [a.label.name for a in wsa.annotations] # biopsy labels\n",
    "\n",
    "        outlines = get_outlines(wsa) # biopsy outlines\n",
    "        try:\n",
    "            wsi = WholeSlideImage(casepaths[\"wsi\"]) # whole slide image\n",
    "        except Exception as e:\n",
    "            errors.append((casepaths[\"wsi\"], e))\n",
    "            continue\n",
    "\n",
    "        for b in range(len(labels)):\n",
    "            outline = outlines[b]\n",
    "            area = get_area(outline, spacing)\n",
    "\n",
    "            biopsy_name = f\"{casename}_b{b}\"\n",
    "            biopsy_df[biopsy_name] = {\n",
    "                \"dir\": d,\n",
    "                \"casename\": casename,\n",
    "                \"height\": int(area[2]),\n",
    "                \"width\": int(area[3]),\n",
    "                \"label\": labels[b],\n",
    "            }\n",
    "\n",
    "            if os.path.exists(os.path.join(dest_biop, f\"{biopsy_name}.png\")):\n",
    "                continue\n",
    "\n",
    "            # Save biopsy with cv2\n",
    "            biopsy = wsi.get_patch(*area, spacing)\n",
    "\n",
    "            cv2.imwrite(os.path.join(dest_biop, f\"{biopsy_name}.png\"), biopsy)\n",
    "\n",
    "    print(\"Errors: \", len(errors))\n",
    "    display(errors)\n",
    "\n",
    "    biopsy_df = pd.DataFrame.from_dict(biopsy_df, orient=\"index\")\n",
    "    biopsy_df[\"pixels\"] = biopsy_df[\"height\"] * biopsy_df[\"width\"]\n",
    "\n",
    "    # Save to csv\n",
    "    biopsy_df.to_csv(os.path.join(destination, \"biopsy_labels.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of casenames with BIG in it\n",
    "print(\"BIG: \", sum([1 for c in casenames if \"BIG\" in c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "errors: 19, each and every BIG tiff\n",
    "\n",
    " clearly something about the BIG tiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_DATASET:\n",
    "    print(\"spacing: \", spacing)\n",
    "\n",
    "    # Use the saved csv to make a prepared dataset, with one folder containing all biopsies with names like 0.png 1.png etc., and a csv with the labels\n",
    "    # Open the csv\n",
    "    df = pd.read_csv(os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", \"biopsy_labels.csv\"), index_col=0)\n",
    "\n",
    "    # For every file in biopsies_s4.0, copy it to biopsies_s4.0_anon, and rename it to the number of the row in the csv (so 0.png, 1.png etc.) (NOT the index)\n",
    "    source = os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", f\"biopsies_s{spacing}\")\n",
    "    destination = os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", f\"biopsies_s{spacing}_anon\")\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "    # Enumerate over the rows in the csv, NOT the index\n",
    "    df_clean = df.copy().reset_index()\n",
    "    for i, row in tqdm(df_clean.iterrows(), total=len(df)):\n",
    "        # Get the biopsy name\n",
    "        biopsy_name = row[\"index\"]\n",
    "        # Get the label\n",
    "        label = row[\"label\"]\n",
    "        # Get the source path\n",
    "        source_path = os.path.join(source, f\"{biopsy_name}.png\")\n",
    "        # Get the destination path\n",
    "        destination_path = os.path.join(destination, f\"{i}.png\")\n",
    "        # Copy the file\n",
    "        os.system(f'copy \"{source_path}\" \"{destination_path}\"')\n",
    "\n",
    "    if not os.path.exists(os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", \"biopsy_labels_anon.csv\")):\n",
    "        # Drop all unnecessary columns, only keeping the index and the label\n",
    "        df_clean = df_clean.drop(columns=[\"dir\", \"casename\", \"height\", \"width\", \"pixels\", \"index\"])\n",
    "\n",
    "        # Map labels to numbers according to the following mapping:\n",
    "        mapping = {\n",
    "            \"wildtype\": 0,\n",
    "            \"overexpression\": 1,\n",
    "            \"nullmutation\": 2,\n",
    "            \"doubleclones\": 3,\n",
    "        }\n",
    "        df_clean[\"label\"] = df_clean[\"label\"].map(mapping)\n",
    "        # It should be an integer\n",
    "        df_clean[\"label\"] = df_clean[\"label\"].astype(int)\n",
    "\n",
    "        # Name the index column \"id\" and then don't save the index\n",
    "        df_clean.index.name = \"id\"\n",
    "        df_clean.to_csv(os.path.join(LANS_BIOP_ROOT, \"dataset_fullsize\", \"biopsy_labels_anon.csv\"))\n",
    "    else:\n",
    "        print(\"biopsy_labels_anon.csv already exists. It's reusable for different spacings, so you don't have to create it again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
